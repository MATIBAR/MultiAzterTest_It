{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------You are going to use Cube Library-----------\n",
      "-----------You are going to use Cube Library-----------\n",
      "Loading latest local model: eu-1.0\n",
      "\tLoading embeddings ... \n",
      "\tLoading tokenization model ...\n",
      "\tLoading lemmatization model ...\n",
      "\tLoading tagger model ...\n",
      "\tLoading parser model ...\n",
      "Model loading complete.\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d13acd240a37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    934\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m \u001b[0mmain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 936\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-d13acd240a37>\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    927\u001b[0m                 \u001b[0;34m\"Ibon tambien va a ir. El es Ibon.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 929\u001b[0;31m         \u001b[0mdocument\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcargador\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_estructure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    930\u001b[0m         \u001b[0mindicators\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indicators\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mprinter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPrinter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindicators\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-d13acd240a37>\u001b[0m in \u001b[0;36mget_estructure\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    887\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtextwithparagraphs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;31m#Getting a unified structure [ [sentences], [sentences], ...]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madapt_nlp_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0madapt_nlp_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-d13acd240a37>\u001b[0m in \u001b[0;36madapt_nlp_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    891\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0madapt_nlp_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m         \u001b[0mma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelAdapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 893\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_analysis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtextwithparagraphs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-d13acd240a37>\u001b[0m in \u001b[0;36mmodel_analysis\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                 \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParagraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#-> paragraph = []\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m                 \u001b[0msequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msequences\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m                     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "import stanfordnlp\n",
    "from cube.api import Cube       # import the Cube object\n",
    "\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "\n",
    "class ModelAdapter:\n",
    "\n",
    "    def __init__(self, model, lib):\n",
    "        # parser\n",
    "        self.model = model\n",
    "        # model_name\n",
    "        self.lib = lib\n",
    "   \n",
    "    def model_analysis(self, text):\n",
    "        d = Document(text) #->data = []\n",
    "        if self.lib.lower() == \"stanford\":\n",
    "            lines = text.split('@')\n",
    "            for line in lines:  #paragraph\n",
    "                p = Paragraph() #-> paragraph = []\n",
    "                if not line.strip() == '':\n",
    "                    doc = self.model(line)\n",
    "                    for sent in doc.sentences:\n",
    "                        s = Sentence()\n",
    "                        sequence = self.sent2sequenceStanford(sent)\n",
    "                        print(sequence)\n",
    "                        s.text = sequence\n",
    "                        for word in sent.words:\n",
    "                            #Por cada palabra de cada sentencia, creamos un objeto Word que contendra los attrs\n",
    "                            w = Word()\n",
    "                            w.index = str(word.index)\n",
    "                            w.text = word.text\n",
    "                            w.lemma = word.lemma\n",
    "                            w.upos = word.upos\n",
    "                            w.xpos = word.xpos\n",
    "                            w.feats = word.feats\n",
    "                            w.governor = word.governor\n",
    "                            w.dependency_relation = word.dependency_relation\n",
    "                            s.word_list.append(w)\n",
    "                            print(str(\n",
    "                                w.index) + \"\\t\" + w.text + \"\\t\" + w.lemma + \"\\t\" + w.upos + \"\\t\" +\n",
    "                                  w.xpos + \"\\t\" + w.feats + \"\\t\" + str(w.governor) + \"\\t\" + str(w.dependency_relation) +\n",
    "                                  \"\\t\")\n",
    "                        p.sentence_list.append(s) #->paragraph.append(s)\n",
    "                    d.paragraph_list.append(p) #->data.append(paragraph)\n",
    "\n",
    "        elif self.lib.lower() == \"cube\":\n",
    "            d = Document(text) #->data = []\n",
    "            lines = text.split('@')\n",
    "            for line in lines:\n",
    "                p = Paragraph() #-> paragraph = []\n",
    "                sequences = self.model(line)\n",
    "                for seq in sequences:\n",
    "                    s = Sentence()\n",
    "                    sequence = self.sent2sequenceCube(seq)\n",
    "                    s.text = sequence\n",
    "                    for entry in seq:\n",
    "                        # Por cada palabra de cada sentencia, creamos un objeto Word que contendra los attrs\n",
    "                        w = Word()\n",
    "                        w.index = str(entry.index)\n",
    "                        w.text = entry.word\n",
    "                        w.lemma = entry.lemma\n",
    "                        w.upos = entry.upos\n",
    "                        w.xpos = entry.xpos\n",
    "                        w.feats = entry.attrs\n",
    "                        w.governor = str(entry.head)\n",
    "                        w.dependency_relation = str(entry.label)\n",
    "                        s.word_list.append(w)\n",
    "                    p.sentence_list.append(s) #->paragraph.append(s)\n",
    "                d.paragraph_list.append(p) #->data.append(paragraph)\n",
    "        return d\n",
    "    \n",
    "\n",
    "    def sent2sequenceStanford(self, sent):\n",
    "        conllword = \"\"\n",
    "        for word in sent.words:\n",
    "            conllword = conllword + \" \" + str(word.text)\n",
    "        return conllword\n",
    "\n",
    "    def sent2sequenceCube(self, sent):\n",
    "        conllword = \"\"\n",
    "        for entry in sent:\n",
    "            conllword = conllword + \" \" + str(entry.word)\n",
    "        return conllword\n",
    "\n",
    "\n",
    "class Document:\n",
    "    def __init__(self, text):\n",
    "        self._text = text\n",
    "        self._paragraph_list = []\n",
    "        self.words_freq = {}\n",
    "        # Indicadores\n",
    "        self.indicators = defaultdict(float)\n",
    "        self.aux_lists = defaultdict(list)\n",
    "\n",
    "    @property\n",
    "    def text(self):\n",
    "        \"\"\" Access text of this document. Example: 'This is a sentence.'\"\"\"\n",
    "        return self._text\n",
    "\n",
    "    @text.setter\n",
    "    def text(self, value):\n",
    "        \"\"\" Set the document's text value. Example: 'This is a sentence.'\"\"\"\n",
    "        self._text = value\n",
    "\n",
    "    @property\n",
    "    def paragraph_list(self):\n",
    "        \"\"\" Access list of sentences for this document. \"\"\"\n",
    "        return self._paragraph_list\n",
    "\n",
    "    @paragraph_list.setter\n",
    "    def paragraph_list(self, value):\n",
    "        \"\"\" Set the list of tokens for this document. \"\"\"\n",
    "        self._paragraph_list = value\n",
    "\n",
    "    def get_indicators(self):\n",
    "        self.indicators['num_sentences'] = self.calculate_num_sentences()\n",
    "        self.indicators['num_words'] = self.calculate_num_words()\n",
    "        self.indicators['num_paragraphs'] = self.calculate_num_paragraphs()\n",
    "        self.analyze()\n",
    "        self.calculate_all_means()\n",
    "        self.calculate_all_std_deviations()\n",
    "        self.calculate_all_incidence()\n",
    "        self.calculate_density()\n",
    "        return self.indicators\n",
    "\n",
    "    def calculate_num_words(self):\n",
    "        num_words = 0\n",
    "        not_punctuation = lambda w: not (len(w.text) == 1 and (not w.text.isalpha()))\n",
    "        for paragraph in self._paragraph_list:\n",
    "            self.aux_lists['sentences_per_paragraph'].append(len(paragraph.sentence_list))  # [1,2,1,...]\n",
    "            for sentence in paragraph.sentence_list:\n",
    "                filterwords = filter(not_punctuation, sentence.word_list)\n",
    "                sum = 0\n",
    "                for word in filterwords:\n",
    "                    num_words += 1\n",
    "                    self.aux_lists['words_length_list'].append(len(word.text))\n",
    "                    self.aux_lists['lemmas_length_list'].append(len(word.lemma))\n",
    "                    sum += 1\n",
    "                self.aux_lists['sentences_length_mean'].append(sum)\n",
    "        return num_words\n",
    "\n",
    "    def calculate_num_paragraphs(self):\n",
    "        return len(self._paragraph_list)\n",
    "\n",
    "    def calculate_num_sentences(self):\n",
    "        num_sentences = 0\n",
    "        for paragraph in self._paragraph_list:\n",
    "            for sentence in paragraph.sentence_list:\n",
    "                num_sentences += 1\n",
    "        return num_sentences\n",
    "\n",
    "    def calculate_left_embeddedness(self, sequences):\n",
    "        list_left_embeddedness = []\n",
    "        for sequence in sequences:\n",
    "            verb_index = 0\n",
    "            main_verb_found = False\n",
    "            left_embeddedness = 0\n",
    "            num_words = 0\n",
    "            for word in sequence.word_list:\n",
    "                if not len(word.text) == 1 or word.text.isalpha():\n",
    "                    if not main_verb_found and word.governor < len(sequence.word_list):\n",
    "                        if self.is_verb(word, sequence):\n",
    "                            verb_index += 1\n",
    "                            if (word.upos == 'VERB' and word.dependency_relation == 'root') or (\n",
    "                                    word.upos == 'AUX' and sequence.word_list[word.governor].dependency_relation == 'root'\n",
    "                                    and sequence.word_list[word.governor].upos == 'VERB'):\n",
    "                                main_verb_found = True\n",
    "                                left_embeddedness = num_words\n",
    "                            if verb_index == 1:\n",
    "                                left_embeddedness = num_words\n",
    "                    num_words += 1\n",
    "            list_left_embeddedness.append(left_embeddedness)\n",
    "        self.indicators['left_embeddedness'] = round(float(np.mean(list_left_embeddedness)), 4)\n",
    "\n",
    "    def count_np_in_sentence(self, sentence):\n",
    "        list_np_indexes = []\n",
    "        for word in sentence.word_list:\n",
    "            if word.upos == 'NOUN' or word.upos == 'PRON' or word.upos == 'PROPN':\n",
    "                if word.dependency_relation in ['fixed', 'flat', 'compound']:\n",
    "                    if word.governor not in list_np_indexes:\n",
    "                        list_np_indexes.append(word.governor)\n",
    "                else:\n",
    "                    if word.index not in list_np_indexes:\n",
    "                        list_np_indexes.append(word.index)\n",
    "        return list_np_indexes\n",
    "\n",
    "    def is_verb(self, word, sequence):\n",
    "        return word.upos == 'VERB' or (word.upos == 'AUX' and sequence.word_list[word.governor - 1].upos != 'VERB')\n",
    "\n",
    "    def is_lexic_word(self, entry, sequence):\n",
    "        return self.is_verb(entry, sequence) or entry.upos == 'NOUN' or entry.upos == 'ADJ' or entry.upos == 'ADV'\n",
    "\n",
    "    def count_decendents(self, sentence, list_np_indexes):\n",
    "        num_modifiers = 0\n",
    "        if len(list_np_indexes) == 0:\n",
    "            return num_modifiers\n",
    "        else:\n",
    "            new_list_indexes = []\n",
    "            for entry in sentence.word_list:\n",
    "                if entry.governor in list_np_indexes and entry.has_modifier():\n",
    "                    new_list_indexes.append(entry.index)\n",
    "                    num_modifiers += 1\n",
    "            return num_modifiers + self.count_decendents(sentence, new_list_indexes)\n",
    "\n",
    "    def get_num_hapax_legomena(self):\n",
    "        num_hapax_legonema = 0\n",
    "        for word, frecuencia in self.words_freq.items():\n",
    "            if frecuencia == 1:\n",
    "                num_hapax_legonema += 1\n",
    "        return num_hapax_legonema\n",
    "\n",
    "    def calculate_honore(self):\n",
    "        n = self.indicators['num_words']\n",
    "        v = len(self.aux_lists['different_forms'])\n",
    "        v1 = self.get_num_hapax_legomena()\n",
    "        self.indicators['honore'] = round(100 * ((np.log10(n)) / (1 - (v1 / v))), 4)\n",
    "\n",
    "    def calculate_maas(self):\n",
    "        n = self.indicators['num_words']\n",
    "        v = len(self.aux_lists['different_forms'])\n",
    "        self.indicators['maas'] = round((np.log10(n) - np.log10(v)) / (np.log10(v) ** 2), 4)\n",
    "\n",
    "    def analyze(self):\n",
    "        i = self.indicators\n",
    "        # num_np_list = []\n",
    "        # decendents_total = 0\n",
    "        subordinadas_labels = ['csubj', 'csubj:pass', 'ccomp', 'xcomp', 'advcl', 'acl', 'acl:relcl']\n",
    "\n",
    "        for p in self.paragraph_list:\n",
    "            self.calculate_left_embeddedness(p.sentence_list)\n",
    "            for s in p.sentence_list:\n",
    "                # vp_indexes = self.count_np_in_sentence(s)\n",
    "                # num_np_list.append(len(vp_indexes))\n",
    "                # decendents_total += self.count_decendents(s, vp_indexes)\n",
    "                i['prop'] = 0\n",
    "                numPunct = 0\n",
    "                for w in s.word_list:\n",
    "                    if self.is_lexic_word(w, s):\n",
    "                        i['num_lexic_words'] += 1\n",
    "                    if w.upos == 'NOUN':\n",
    "                        i['num_noun'] += 1\n",
    "                    if w.upos == 'ADJ':\n",
    "                        i['num_adj'] += 1\n",
    "                    if w.upos == 'ADV':\n",
    "                        i['num_adv'] += 1\n",
    "                    if self.is_verb(w, s):\n",
    "                        i['num_verb'] += 1\n",
    "                    if w.text.lower() not in self.aux_lists['different_forms']:\n",
    "                        self.aux_lists['different_forms'].append(w.text.lower())\n",
    "                    if w.text.lower() not in self.words_freq:\n",
    "                        self.words_freq[w.text.lower()] = 1\n",
    "                    else:\n",
    "                        self.words_freq[w.text.lower()] = self.words_freq.get(w.text.lower()) + 1\n",
    "                    if w.dependency_relation in subordinadas_labels:\n",
    "                        i['num_subord'] += 1\n",
    "                        # Numero de sentencias subordinadas relativas\n",
    "                        if w.dependency_relation == 'acl:relcl':\n",
    "                            i['num_rel_subord'] += 1\n",
    "                    if w.upos == 'PUNCT':\n",
    "                        numPunct += 1\n",
    "                    if w.dependency_relation == 'conj' or w.dependency_relation == 'csubj' or w.dependency_relation == 'csubj:pass' or w.dependency_relation == 'ccomp' or w.dependency_relation == 'xcomp' or w.dependency_relation == 'advcl' or w.dependency_relation == 'acl' or w.dependency_relation == 'acl:relcl':\n",
    "                        i['prop'] += 1\n",
    "                    atributos = w.feats.split('|')\n",
    "                    if 'VerbForm=Ger' in atributos:\n",
    "                        i['num_ger'] += 1\n",
    "                    if 'VerbForm=Inf' in atributos:\n",
    "                        i['num_inf'] += 1\n",
    "                    if 'Mood=Imp' in atributos:\n",
    "                        i['num_impera'] += 1\n",
    "                    if 'PronType=Prs' in atributos:\n",
    "                        i['num_personal_pronouns'] += 1\n",
    "                        if 'Person=1' in atributos:\n",
    "                            i['num_first_pers_pron'] += 1\n",
    "                            if 'Number=Sing' in atributos:\n",
    "                                i['num_first_pers_sing_pron'] += 1\n",
    "                        elif 'Person=3' in atributos:\n",
    "                            i['num_third_pers_pron'] += 1\n",
    "                i['num_total_prop'] = i['num_total_prop'] + i['prop']\n",
    "                self.aux_lists['prop_per_sentence'].append(i['prop'])\n",
    "                self.aux_lists['punct_per_sentence'].append(numPunct)\n",
    "        # i['num_decendents_noun_phrase'] = round(decendents_total / sum(num_np_list), 4)\n",
    "        i['num_different_forms'] = len(self.aux_lists['different_forms'])\n",
    "        self.calculate_honore()\n",
    "        self.calculate_maas()\n",
    "\n",
    "    def calculate_all_means(self):\n",
    "        i = self.indicators\n",
    "        i['sentences_per_paragraph_mean'] = round(float(np.mean(self.aux_lists['sentences_per_paragraph'])), 4)\n",
    "        i['sentences_length_mean'] = round(float(np.mean(self.aux_lists['sentences_length_mean'])), 4)\n",
    "        i['words_length_mean'] = round(float(np.mean(self.aux_lists['words_length_list'])), 4)\n",
    "        i['lemmas_length_mean'] = round(float(np.mean(self.aux_lists['lemmas_length_list'])), 4)\n",
    "        i['mean_propositions_per_sentence'] = round(float(np.mean(self.aux_lists['prop_per_sentence'])), 4)\n",
    "        i['num_punct_marks_per_sentence'] = round(float(np.mean(self.aux_lists['punct_per_sentence'])), 4)\n",
    "\n",
    "    def calculate_all_std_deviations(self):\n",
    "        i = self.indicators\n",
    "        i['sentences_per_paragraph_std'] = round(float(np.std(self.aux_lists['sentences_per_paragraph'])), 4)\n",
    "        i['sentences_length_std'] = round(float(np.std(self.aux_lists['sentences_length_mean'])), 4)\n",
    "        i['words_length_std'] = round(float(np.std(self.aux_lists['words_length_list'])), 4)\n",
    "        i['lemmas_length_std'] = round(float(np.std(self.aux_lists['lemmas_length_list'])), 4)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_incidence(indicador, num_words):\n",
    "        return round(((1000 * indicador) / num_words), 4)\n",
    "\n",
    "    def calculate_all_incidence(self):\n",
    "        i = self.indicators\n",
    "        n = i['num_words']\n",
    "        i['num_sentences_incidence'] = self.get_incidence(i['num_sentences'], n)\n",
    "        i['num_paragraphs_incidence'] = self.get_incidence(i['num_paragraphs'], n)\n",
    "        i['num_impera_incidence'] = self.get_incidence(i['num_impera'], n)\n",
    "        i['num_personal_pronouns_incidence'] = self.get_incidence(i['num_personal_pronouns'], n)\n",
    "        i['num_first_pers_pron_incidence'] = self.get_incidence(i['num_first_pers_pron'], n)\n",
    "        i['num_first_pers_sing_pron_incidence'] = self.get_incidence(i['num_first_pers_sing_pron'], n)\n",
    "        i['num_third_pers_pron_incidence'] = self.get_incidence(i['num_third_pers_pron'], n)\n",
    "        i['gerund_density_incidence'] = self.get_incidence(i['num_ger'], n)\n",
    "        i['infinitive_density_incidence'] = self.get_incidence(i['num_inf'], n)\n",
    "        i['num_subord_incidence'] = self.get_incidence(i['num_subord'], n)\n",
    "        i['num_rel_subord_incidence'] = self.get_incidence(i['num_rel_subord'], n)\n",
    "\n",
    "    def calculate_density(self):\n",
    "        i = self.indicators\n",
    "        i['lexical_density'] = round(i['num_lexic_words'] / i['num_words'], 4)\n",
    "        i['noun_density'] = round(i['num_noun'] / i['num_words'], 4)\n",
    "        i['verb_density'] = round(i['num_verb'] / i['num_words'], 4)\n",
    "        i['adj_density'] = round(i['num_adj'] / i['num_words'], 4)\n",
    "        i['adv_density'] = round(i['num_adv'] / i['num_words'], 4)\n",
    "\n",
    "\n",
    "class Paragraph:\n",
    "\n",
    "    def __init__(self):\n",
    "        self._sentence_list = []\n",
    "\n",
    "    @property\n",
    "    def sentence_list(self):\n",
    "        \"\"\" Access list of sentences for this document. \"\"\"\n",
    "        return self._sentence_list\n",
    "\n",
    "    @sentence_list.setter\n",
    "    def sentence_list(self, value):\n",
    "        \"\"\" Set the list of tokens for this document. \"\"\"\n",
    "        self.sentence_list = value\n",
    "\n",
    "\n",
    "class Sentence:\n",
    "\n",
    "    def __init__(self):\n",
    "        self._word_list = []\n",
    "        self.text = None\n",
    "\n",
    "    @property\n",
    "    def word_list(self):\n",
    "        \"\"\" Access list of words for this sentence. \"\"\"\n",
    "        return self._word_list\n",
    "\n",
    "    @word_list.setter\n",
    "    def word_list(self, value):\n",
    "        \"\"\" Set the list of words for this sentence. \"\"\"\n",
    "        self._word_list = value\n",
    "\n",
    "    def print(self):\n",
    "        for words in self.word_list:\n",
    "            print(words.text)\n",
    "\n",
    "\n",
    "class Word:\n",
    "    def __init__(self):\n",
    "        self._index = None\n",
    "        self._text = None\n",
    "        self._lemma = None\n",
    "        self._upos = None\n",
    "        self._xpos = None\n",
    "        self._feats = None\n",
    "        self._governor = None\n",
    "        self._dependency_relation = None\n",
    "\n",
    "    @property\n",
    "    def dependency_relation(self):\n",
    "        \"\"\" Access dependency relation of this word. Example: 'nmod'\"\"\"\n",
    "        return self._dependency_relation\n",
    "\n",
    "    @dependency_relation.setter\n",
    "    def dependency_relation(self, value):\n",
    "        \"\"\" Set the word's dependency relation value. Example: 'nmod'\"\"\"\n",
    "        self._dependency_relation = value\n",
    "\n",
    "    @property\n",
    "    def lemma(self):\n",
    "        \"\"\" Access lemma of this word. \"\"\"\n",
    "        return self._lemma\n",
    "\n",
    "    @lemma.setter\n",
    "    def lemma(self, value):\n",
    "        \"\"\" Set the word's lemma value. \"\"\"\n",
    "        self._lemma = value\n",
    "\n",
    "    @property\n",
    "    def governor(self):\n",
    "        \"\"\" Access governor of this word. \"\"\"\n",
    "        return self._governor\n",
    "\n",
    "    @governor.setter\n",
    "    def governor(self, value):\n",
    "        \"\"\" Set the word's governor value. \"\"\"\n",
    "        self._governor = value\n",
    "\n",
    "    @property\n",
    "    def pos(self):\n",
    "        \"\"\" Access (treebank-specific) part-of-speech of this word. Example: 'NNP'\"\"\"\n",
    "        return self._xpos\n",
    "\n",
    "    @pos.setter\n",
    "    def pos(self, value):\n",
    "        \"\"\" Set the word's (treebank-specific) part-of-speech value. Example: 'NNP'\"\"\"\n",
    "        self._xpos = value\n",
    "\n",
    "    @property\n",
    "    def text(self):\n",
    "        \"\"\" Access text of this word. Example: 'The'\"\"\"\n",
    "        return self._text\n",
    "\n",
    "    @text.setter\n",
    "    def text(self, value):\n",
    "        \"\"\" Set the word's text value. Example: 'The'\"\"\"\n",
    "        self._text = value\n",
    "\n",
    "    @property\n",
    "    def xpos(self):\n",
    "        \"\"\" Access treebank-specific part-of-speech of this word. Example: 'NNP'\"\"\"\n",
    "        return self._xpos\n",
    "\n",
    "    @xpos.setter\n",
    "    def xpos(self, value):\n",
    "        \"\"\" Set the word's treebank-specific part-of-speech value. Example: 'NNP'\"\"\"\n",
    "        self._xpos = value\n",
    "\n",
    "    @property\n",
    "    def upos(self):\n",
    "        \"\"\" Access universal part-of-speech of this word. Example: 'DET'\"\"\"\n",
    "        return self._upos\n",
    "\n",
    "    @upos.setter\n",
    "    def upos(self, value):\n",
    "        \"\"\" Set the word's universal part-of-speech value. Example: 'DET'\"\"\"\n",
    "        self._upos = value\n",
    "\n",
    "    @property\n",
    "    def feats(self):\n",
    "        \"\"\" Access morphological features of this word. Example: 'Gender=Fem'\"\"\"\n",
    "        return self._feats\n",
    "\n",
    "    @feats.setter\n",
    "    def feats(self, value):\n",
    "        \"\"\" Set this word's morphological features. Example: 'Gender=Fem'\"\"\"\n",
    "        self._feats = value\n",
    "\n",
    "    @property\n",
    "    def parent_token(self):\n",
    "        \"\"\" Access the parent token of this word. \"\"\"\n",
    "        return self._parent_token\n",
    "\n",
    "    @parent_token.setter\n",
    "    def parent_token(self, value):\n",
    "        \"\"\" Set this word's parent token. \"\"\"\n",
    "        self._parent_token = value\n",
    "\n",
    "    @property\n",
    "    def index(self):\n",
    "        \"\"\" Access index of this word. \"\"\"\n",
    "        return self._index\n",
    "\n",
    "    @index.setter\n",
    "    def index(self, value):\n",
    "        \"\"\" Set the word's index value. \"\"\"\n",
    "        self._index = value\n",
    "\n",
    "    def has_modifier(self):\n",
    "        # nominal head may be associated with different types of modifiers and function words\n",
    "        return True if self.dependency_relation in ['nmod', 'nmod:poss', 'appos', 'amod', 'nummod', 'acl', 'acl:relcl', 'det', 'clf',\n",
    "                                       'case'] else False\n",
    "\n",
    "    def __repr__(self):\n",
    "        features = ['index', 'text', 'lemma', 'upos', 'xpos', 'feats', 'governor', 'dependency_relation']\n",
    "        feature_str = \";\".join([\"{}={}\".format(k, getattr(self, k)) for k in features if getattr(self, k) is not None])\n",
    "\n",
    "        return f\"<{self.__class__.__name__} {feature_str}>\"\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import csv\n",
    "\n",
    "\n",
    "class Printer:\n",
    "\n",
    "    def __init__(self, indicators):\n",
    "        self.indicators = indicators\n",
    "\n",
    "    def print_info(self):\n",
    "        i = self.indicators\n",
    "        print(\"------------------------------------------------------------------------------\")\n",
    "        # print('Level of difficulty: ' + prediction[0].title())\n",
    "        print(\"------------------------------------------------------------------------------\")\n",
    "        print('Number of words (total): ' + str(i['num_words']))\n",
    "        # The number of distints lower and alfabetic words\n",
    "        print(\"Number of distinct words (total): \" + str(i['num_different_forms']))\n",
    "        print('Number of words with punctuation (total): ' + str(i['num_words_with_punct']))\n",
    "\n",
    "        print(\"Number of paragraphs (total): \" + str(i['num_paragraphs']))\n",
    "        print(\"Number of paragraphs (incidence per 1000 words): \" + str(i['num_paragraphs_incidence']))\n",
    "        print('Number of sentences (total): ' + str(i['num_sentences']))\n",
    "        print('Number of sentences (incidence per 1000 words): ' + str(i['num_sentences_incidence']))\n",
    "\n",
    "        # Numero de frases en un parrafo (media)\n",
    "        print(\"Length of paragraphs (mean): \" + str(i['sentences_per_paragraph_mean']))\n",
    "        # Numero de frases en un parrafo (desv. Tipica)\n",
    "        print(\"Standard deviation of length of paragraphs: \" + str(i['sentences_per_paragraph_std']))\n",
    "\n",
    "        print(\"Number of words (length) in sentences (mean): \" + str(i['sentences_length_mean']))\n",
    "        print(\"Number of words (length) in sentences (standard deviation): \" + str(i['sentences_length_std']))\n",
    "\n",
    "        print(\"Number of words (length) of sentences without stopwords (mean): \" + str(\n",
    "            i['sentences_length_no_stopwords_mean']))\n",
    "        print(\"Number of words (length) of sentences without stopwords (standard deviation): \" + str(\n",
    "            i['sentences_length_no_stopwords_std']))\n",
    "\n",
    "        print('Mean number of syllables (length) in words: ' + str(i['num_syllables_words_mean']))\n",
    "        print('Standard deviation of the mean number of syllables in words: ' + str(i['num_syllables_words_std']))\n",
    "\n",
    "        print(\"Mean number of letters (length) in words: \" + str(i['words_length_mean']))\n",
    "        print(\"Standard deviation of number of letters in words: \" + str(i['words_length_std']))\n",
    "\n",
    "        print(\"Mean number of letters (length) in words without stopwords: \" + str(i['words_length_no_stopwords_mean']))\n",
    "        print(\"Standard deviation of the mean number of letter in words without stopwords: \" + str(\n",
    "            i['words_length_no_stopwords_std']))\n",
    "\n",
    "        print(\"Mean number of letters (length) in lemmas: \" + str(i['lemmas_length_mean']))\n",
    "        print(\"Standard deviation of letters (length) in lemmas: \" + str(i['lemmas_length_std']))\n",
    "\n",
    "        print('Lexical Density: ' + str(i['lexical_density']))\n",
    "        print(\"Noun Density: \" + str(i['noun_density']))\n",
    "        print(\"Verb Density: \" + str(i['verb_density']))\n",
    "        print(\"Adjective Density: \" + str(i['adj_density']))\n",
    "        print(\"Adverb Density: \" + str(i['adv_density']))\n",
    "\n",
    "        # Simple TTR (Type-Token Ratio)\n",
    "        print('STTR (Simple Type-Token Ratio) : ' + str(i['simple_ttr']))\n",
    "        # Content TTR (Content Type-Token Ratio)\n",
    "        print('CTTR (Content Type-Token Ratio): ' + str(i['content_ttr']))\n",
    "        # NTTR (Noun Type-Token Ratio)\n",
    "        print('NTTR (Noun Type-Token Ratio): ' + str(i['nttr']))\n",
    "        # VTTR (Verb Type-Token Ratio)(incidence per 1000 words)\n",
    "        print('VTTR (Verb Type-Token Ratio): ' + str(i['vttr']))\n",
    "\n",
    "        # AdjTTR (Adj Type-Token Ratio)\n",
    "        print('AdjTTR (Adj Type-Token Ratio): ' + str(i['adj_ttr']))\n",
    "        # AdvTTR (Adv Type-Token Ratio)\n",
    "        print('AdvTTR (Adv Type-Token Ratio): ' + str(i['adv_ttr']))\n",
    "\n",
    "        # Lemma Simple TTR (Type-Token Ratio)\n",
    "        print('LSTTR (Lemma Simple Type-Token Ratio): ' + str(i['lemma_ttr']))\n",
    "        # Lemma Content TTR (Content Type-Token Ratio)\n",
    "        print('LCTTR (Lemma Content Type-Token Ratio): ' + str(i['lemma_content_ttr']))\n",
    "        # LNTTR (Lemma Noun Type-Token Ratio)\n",
    "        print('LNTTR (Lemma Noun Type-Token Ratio) ' + str(i['lemma_nttr']))\n",
    "        # LVTTR (Lemma Verb Type-Token Ratio)\n",
    "        print('LVTTR (Lemma Verb Type-Token Ratio): ' + str(i['lemma_vttr']))\n",
    "        # Lemma AdjTTR (Lemma Adj Type-Token Ratio)\n",
    "        print('LAdjTTR (Lemma Adj Type-Token Ratio): ' + str(i['lemma_adj_ttr']))\n",
    "        # Lemma AdvTTR (Lemma Adv Type-Token Ratio)\n",
    "        print('LAdvTTR (Lemma Adv Type-Token Ratio): ' + str(i['lemma_adv_ttr']))\n",
    "\n",
    "        # Honore\n",
    "        print('Honore Lexical Density: ' + str(i['honore']))\n",
    "        # Maas\n",
    "        print('Maas Lexical Density: ' + str(i['maas']))\n",
    "        # MTLD\n",
    "        print('Measure of Textual Lexical Diversity (MTLD): ' + str(i['mtld']))\n",
    "\n",
    "        # Flesch-Kincaid grade level =0.39 * (n.º de words/nº de frases) + 11.8 * (n.º de silabas/numero de words) – 15.59)\n",
    "        print(\"Flesch-Kincaid Grade level: \" + str(i['flesch_kincaid']))\n",
    "        # Flesch readability ease=206.835-1.015(n.º de words/nº de frases)-84.6(n.º de silabas/numero de words)\n",
    "        print(\"Flesch readability ease: \" + str(i['flesch']))\n",
    "\n",
    "        print(\"Dale-Chall readability formula: \" + str(i['dale_chall']))\n",
    "        print(\"Simple Measure Of Gobbledygook (SMOG) grade: \" + str(i['smog']))\n",
    "\n",
    "        print(\"Number of verbs in past tense: \" + str(i['num_past']))\n",
    "        print(\"Number of verbs in past tense (incidence per 1000 words): \" + str(i['num_past_incidence']))\n",
    "        print(\"Number of verbs in present tense: \" + str(i['num_pres']))\n",
    "        print(\"Number of verbs in present tense (incidence per 1000 words): \" + str(i['num_pres_incidence']))\n",
    "        print(\"Number of verbs in future tense: \" + str(i['num_future']))\n",
    "        print(\"Number of verbs in future tense (incidence per 1000 words): \" + str(i['num_future_incidence']))\n",
    "\n",
    "        # Numero de verbos en modo indicativo\n",
    "        print(\"Number of verbs in indicative mood: \" + str(i['num_indic']))\n",
    "        print(\"Number of verbs in indicative mood (incidence per 1000 words): \" + str(i['num_indic_incidence']))\n",
    "        # Numero de verbos en modo imperativo\n",
    "        print(\"Number of verbs in imperative mood: \" + str(i['num_impera']))\n",
    "        print(\"Number of verbs in imperative mood (incidence per 1000 words): \" + str(i['num_impera_incidence']))\n",
    "        # Numero de verbos en pasado que son irregulares (total)\n",
    "        print(\"Number of irregular verbs in past tense: \" + str(i['num_past_irregular']))\n",
    "        # Numero de verbos en pasado que son irregulares (incidencia 1000 words)\n",
    "        print(\"Number of irregular verbs in past tense (incidence per 1000 words): \" + str(\n",
    "            i['num_past_irregular_incidence']))\n",
    "        # Porcentaje de verbos en pasado que son irregulares sobre total de verbos en pasado\n",
    "        print(\"Mean of irregular verbs in past tense in relation to the number of verbs in past tense: \" + str(\n",
    "            i['num_past_irregular_mean']))\n",
    "        # Number of personal pronouns\n",
    "        print(\"Number of personal pronouns: \" + str(i['num_personal_pronouns']))\n",
    "        # Incidence score of pronouns (per 1000 words)\n",
    "        print(\"Incidence score of pronouns (per 1000 words): \" + str(i['num_personal_pronouns_incidence']))\n",
    "        # Number of pronouns in first person\n",
    "        print(\"Number of pronouns in first person: \" + str(i['num_first_pers_pron']))\n",
    "        # Incidence score of pronouns in first person  (per 1000 words)\n",
    "        print(\n",
    "            \"Incidence score of pronouns in first person  (per 1000 words): \" + str(i['num_first_pers_pron_incidence']))\n",
    "        # Number of pronouns in first person singular\n",
    "        print(\"Number of pronouns in first person singular: \" + str(i['num_first_pers_sing_pron']))\n",
    "        # Incidence score of pronouns in first person singular (per 1000 words)\n",
    "        print(\"Incidence score of pronouns in first person singular (per 1000 words): \" + str(\n",
    "            i['num_first_pers_sing_pron_incidence']))\n",
    "        # Number of pronouns in third person\n",
    "        print(\"Number of pronouns in third person: \" + str(i['num_third_pers_pron']))\n",
    "        # Incidence score of pronouns in third person (per 1000 words)\n",
    "        print(\n",
    "            \"Incidence score of pronouns in third person (per 1000 words): \" + str(i['num_third_pers_pron_incidence']))\n",
    "\n",
    "        print('Minimum word frequency per sentence (mean): ' + str(i['min_wf_per_sentence']))\n",
    "        print('Number of rare nouns (wordfrecuency<=4): ' + str(i['num_rare_nouns_4']))\n",
    "        print('Number of rare nouns (wordfrecuency<=4) (incidence per 1000 words): ' + str(\n",
    "            i['num_rare_nouns_4_incidence']))\n",
    "        print('Number of rare adjectives (wordfrecuency<=4): ' + str(i['num_rare_adj_4']))\n",
    "        print('Number of rare adjectives (wordfrecuency<=4) (incidence per 1000 words): ' + str(\n",
    "            i['num_rare_adj_4_incidence']))\n",
    "        print('Number of rare verbs (wordfrecuency<=4): ' + str(i['num_rare_verbs_4']))\n",
    "        print('Number of rare verbs (wordfrecuency<=4) (incidence per 1000 words): ' + str(\n",
    "            i['num_rare_verbs_4_incidence']))\n",
    "        print('Number of rare adverbs (wordfrecuency<=4): ' + str(i['num_rare_advb_4']))\n",
    "        print('Number of rare adverbs (wordfrecuency<=4) (incidence per 1000 words): ' + str(\n",
    "            i['num_rare_advb_4_incidence']))\n",
    "        print('Number of rare content words (wordfrecuency<=4): ' + str(i['num_rare_words_4']))\n",
    "        print('Number of rare content words (wordfrecuency<=4) (incidence per 1000 words): ' + str(\n",
    "            i['num_rare_words_4_incidence']))\n",
    "        print('Number of distinct rare content words (wordfrecuency<=4): ' + str(i['num_dif_rare_words_4']))\n",
    "        print('Number of distinct rare content words (wordfrecuency<=4) (incidence per 1000 words): ' + str(\n",
    "            i['num_dif_rare_words_4_incidence']))\n",
    "        # The average of rare lexical words (whose word frequency value is less than 4) with respect to the total of lexical words\n",
    "        print('Mean of rare lexical words (word frequency <= 4): ' + str(i['mean_rare_4']))\n",
    "        # The average of distinct rare lexical words (whose word frequency value is less than 4) with respect to the total of distinct lexical words\n",
    "        print('Mean of distinct rare lexical words (word frequency <= 4): ' + str(i['mean_distinct_rare_4']))\n",
    "\n",
    "        print('Number of A1 vocabulary in the text: ' + str(i['num_a1_words']))\n",
    "        print('Incidence score of A1 vocabulary  (per 1000 words): ' + str(i['num_a1_words_incidence']))\n",
    "        print('Number of A2 vocabulary in the text: ' + str(i['num_a2_words']))\n",
    "        print('Incidence score of A2 vocabulary  (per 1000 words): ' + str(i['num_a2_words_incidence']))\n",
    "        print('Number of B1 vocabulary in the text: ' + str(i['num_b1_words']))\n",
    "        print('Incidence score of B1 vocabulary  (per 1000 words): ' + str(i['num_b1_words_incidence']))\n",
    "        print('Number of B2 vocabulary in the text: ' + str(i['num_b2_words']))\n",
    "        print('Incidence score of B2 vocabulary  (per 1000 words): ' + str(i['num_b2_words_incidence']))\n",
    "        print('Number of C1 vocabulary in the text: ' + str(i['num_c1_words']))\n",
    "        print('Incidence score of C1 vocabulary  (per 1000 words): ' + str(i['num_c1_words_incidence']))\n",
    "        print('Number of content words not in A1-C1 vocabulary: ' + str(i['num_content_words_not_a1_c1_words']))\n",
    "        print('Incidence score of content words not in A1-C1 vocabulary (per 1000 words): ' + str(\n",
    "            i['num_content_words_not_a1_c1_words_incidence']))\n",
    "\n",
    "        print('Number of content words: ' + str(i['num_lexic_words']))\n",
    "        print('Number of content words (incidence per 1000 words): ' + str(i['num_lexic_words_incidence']))\n",
    "        print(\"Number of nouns: \" + str(i['num_noun']))\n",
    "        print(\"Number of nouns (incidence per 1000 words): \" + str(i['num_noun_incidence']))\n",
    "        print(\"Number of adjectives: \" + str(i['num_adj']))\n",
    "        print(\"Number of adjectives (incidence per 1000 words): \" + str(i['num_adj_incidence']))\n",
    "        print(\"Number of adverbs: \" + str(i['num_adv']))\n",
    "        print(\"Number of adverbs (incidence per 1000 words): \" + str(i['num_adv_incidence']))\n",
    "        print(\"Number of verbs: \" + str(i['num_verb']))\n",
    "        print(\"Number of verbs (incidence per 1000 words): \" + str(i['num_verb_incidence']))\n",
    "        # Left-Embeddedness\n",
    "        print(\n",
    "            \"Left embeddedness (Mean of number of words before the main verb) (SYNLE): \" + str(i['left_embeddedness']))\n",
    "\n",
    "        print(\"Number of decendents per noun phrase (mean): \" + str(i['num_decendents_noun_phrase']))\n",
    "        print(\"Number of modifiers per noun phrase (mean) (SYNNP): \" + str(i['num_modifiers_noun_phrase']))\n",
    "        print(\"Mean of the number of levels of dependency tree (Depth): \" + str(i['mean_depth_per_sentence']))\n",
    "\n",
    "        # Numero de sentencias subordinadas\n",
    "        print(\"Number of subordinate clauses: \" + str(i['num_subord']))\n",
    "        # Numero de sentencias subordinadas (incidence per 1000 words)\n",
    "        print(\"Number of subordinate clauses (incidence per 1000 words): \" + str(i['num_subord_incidence']))\n",
    "        # Numero de sentencias subordinadas relativas\n",
    "        print(\"Number of relative subordinate clauses: \" + str(i['num_rel_subord']))\n",
    "        # Numero de sentencias subordinadas relativas (incidence per 1000 words)\n",
    "        print(\n",
    "            \"Number of relative subordinate clauses (incidence per 1000 words): \" + str(i['num_rel_subord_incidence']))\n",
    "        # Marcas de puntuacion por sentencia (media)\n",
    "        print(\"Punctuation marks per sentence (mean): \" + str(i['num_punct_marks_per_sentence']))\n",
    "        print('Number of propositions: ' + str(i['num_total_prop']))\n",
    "        # Mean of the number of propositions per sentence\n",
    "        print('Mean of the number of propositions per sentence: ' + str(i['mean_propositions_per_sentence']))\n",
    "\n",
    "        print('Mean of the number of VPs per sentence: ' + str(i['mean_vp_per_sentence']))\n",
    "        print('Mean of the number of NPs per sentence: ' + str(i['mean_np_per_sentence']))\n",
    "        print('Noun phrase density, incidence (DRNP): ' + str(i['noun_phrase_density_incidence']))\n",
    "        print('Verb phrase density, incidence (DRVP): ' + str(i['verb_phrase_density_incidence']))\n",
    "        # Numero de verbos en pasiva (total)\n",
    "        print(\"Number of passive voice verbs: \" + str(i['num_pass']))\n",
    "        # Numero de verbos en pasiva (incidence per 1000 words)\n",
    "        print(\"Number of passive voice verbs (incidence per 1000 words): \" + str(i['num_pass_incidence']))\n",
    "        # Porcentaje de verbos en pasiva\n",
    "        print(\"Mean of passive voice verbs: \" + str(i['num_pass_mean']))\n",
    "        # Numero de verbos en pasiva que no tienen agente\n",
    "        print(\"Number of agentless passive voice verbs: \" + str(i['num_agentless']))\n",
    "        print('Agentless passive voice density, incidence (DRPVAL): ' + str(i['agentless_passive_density_incidence']))\n",
    "        print(\"Number of negative words: \" + str(i['num_neg']))\n",
    "        print('Negation density, incidence (DRNEG): ' + str(i['negation_density_incidence']))\n",
    "        print(\"Number of verbs in gerund form: \" + str(i['num_ger']))\n",
    "        print('Gerund density, incidence (DRGERUND): ' + str(i['gerund_density_incidence']))\n",
    "        print(\"Number of verbs in infinitive form: \" + str(i['num_inf']))\n",
    "        print('Infinitive density, incidence (DRINF): ' + str(i['infinitive_density_incidence']))\n",
    "\n",
    "        # Ambigüedad de una palabra (polysemy in WordNet)\n",
    "        print('Mean values of polysemy in the WordNet lexicon: ' + str(i['polysemic_index']))\n",
    "        # Nivel de abstracción (hypernym in WordNet)\n",
    "        print('Mean hypernym values of verbs in the WordNet lexicon: ' + str(i['hypernymy_verbs_index']))\n",
    "        print('Mean hypernym values of nouns in the WordNet lexicon: ' + str(i['hypernymy_nouns_index']))\n",
    "        print('Mean hypernym values of nouns and verbs in the WordNet lexicon: ' + str(i['hypernymy_index']))\n",
    "\n",
    "        # Textbase. Referential cohesion\n",
    "        print('Noun overlap, adjacent sentences, binary, mean (CRFNOl): ' + str(i['noun_overlap_adjacent']))\n",
    "        print('Noun overlap, all of the sentences in a paragraph or text, binary, mean (CRFNOa): ' + str(\n",
    "            i['noun_overlap_all']))\n",
    "        print('Argument overlap, adjacent sentences, binary, mean (CRFAOl): ' + str(i['argument_overlap_adjacent']))\n",
    "        print('Argument overlap, all of the sentences in a paragraph or text, binary, mean (CRFAOa): ' + str(\n",
    "            i['argument_overlap_all']))\n",
    "        print('Stem overlap, adjacent sentences, binary, mean (CRFSOl): ' + str(i['stem_overlap_adjacent']))\n",
    "        print('Stem overlap, all of the sentences in a paragraph or text, binary, mean (CRFSOa): ' + str(\n",
    "            i['stem_overlap_all']))\n",
    "        print('Content word overlap, adjacent sentences, proportional, mean (CRFCWO1): ' + str(\n",
    "            i['content_overlap_adjacent_mean']))\n",
    "        print('Content word overlap, adjacent sentences, proportional, standard deviation (CRFCWO1d): ' + str(\n",
    "            i['content_overlap_adjacent_std']))\n",
    "        print('Content word overlap, all of the sentences in a paragraph or text, proportional, mean (CRFCWOa): ' + str(\n",
    "            i['content_overlap_all_mean']))\n",
    "        print(\n",
    "            'Content word overlap, all of the sentences in a paragraph or text, proportional, standard deviation (CRFCWOad): ' + str(\n",
    "                i['content_overlap_all_std']))\n",
    "        # Connectives\n",
    "        print('Number of connectives (incidence per 1000 words): ' + str(i['all_connectives_incidence']))\n",
    "        print('Causal connectives (incidence per 1000 words): ' + str(i['causal_connectives_incidence']))\n",
    "        print('Logical connectives (incidence per 1000 words):  ' + str(i['logical_connectives_incidence']))\n",
    "        print('Adversative/contrastive connectives (incidence per 1000 words): ' + str(\n",
    "            i['adversative_connectives_incidence']))\n",
    "        print('Temporal connectives (incidence per 1000 words):  ' + str(i['temporal_connectives_incidence']))\n",
    "        print('Conditional connectives (incidence per 1000 words): ' + str(i['conditional_connectives_incidence']))\n",
    "\n",
    "'''\n",
    "The aim of this class is the charge of the model with the specific language and nlp library.\n",
    "In addition, it is going to create a unified data structure to obtain the indicators independent of the library \n",
    "and language.\n",
    "'''\n",
    "\n",
    "\n",
    "class NLPCharger:\n",
    "\n",
    "    def __init__(self, language, library):\n",
    "        self.lang = language\n",
    "        self.lib = library\n",
    "        self.text = None\n",
    "        self.textwithparagraphs = None\n",
    "        self.parser = None\n",
    "\n",
    "    '''\n",
    "    Download the respective model depending of the library and language. \n",
    "    '''\n",
    "    def download_model(self):\n",
    "        if self.lib.lower() == \"stanford\":\n",
    "            print(\"-----------You are going to use Stanford library-----------\")\n",
    "            if self.lang.lower() == \"basque\":\n",
    "                print(\"-------------You are going to use Basque model-------------\")\n",
    "                MODELS_DIR = '/home/kepa/eu'\n",
    "                stanfordnlp.download('eu', MODELS_DIR)  # Download the Basque models\n",
    "            elif self.lang.lower() == \"english\":\n",
    "                print(\"-------------You are going to use English model-------------\")\n",
    "                MODELS_DIR = '/home/kepa/en'\n",
    "                print(\"-------------Downloading Stanford Basque model-------------\")\n",
    "                stanfordnlp.download('en', MODELS_DIR)  # Download the Basque models\n",
    "            elif self.lang.lower() == \"spanish\":\n",
    "                print(\"-------------You are going to use Spanish model-------------\")\n",
    "                MODELS_DIR = '/home/kepa/es'\n",
    "                stanfordnlp.download('es', MODELS_DIR) # Download the English models\n",
    "            else:\n",
    "                print(\"........You cannot use this language...........\")\n",
    "        elif self.lib.lower() == \"cube\":\n",
    "            print(\"-----------You are going to use Cube Library-----------\")\n",
    "        else:\n",
    "            print(\"You cannot use this library. Introduce a valid library (Cube or Stanford)\")\n",
    "\n",
    "            \n",
    "            \n",
    "    '''\n",
    "    load model in parser object \n",
    "    '''\n",
    "    def load_model(self):\n",
    "        if self.lib.lower() == \"stanford\":\n",
    "            print(\"-----------You are going to use Stanford library-----------\")\n",
    "            if self.lang.lower() == \"basque\":\n",
    "                print(\"-------------You are going to use Basque model-------------\")\n",
    "                config = {'processors': 'tokenize,pos,lemma,depparse',  # Comma-separated list of processors to use\n",
    "                           'lang': 'eu',  # Language code for the language to build the Pipeline in\n",
    "                           'tokenize_model_path': '/home/kepa/eu/eu_bdt_models/eu_bdt_tokenizer.pt',\n",
    "                           # Processor-specific arguments are set with keys \"{processor_name}_{argument_name}\"\n",
    "                           'pos_model_path': '/home/kepa/eu/eu_bdt_models/eu_bdt_tagger.pt',\n",
    "                           'pos_pretrain_path': '/home/kepa/eu/eu_bdt_models/eu_bdt.pretrain.pt',\n",
    "                           'lemma_model_path': '/home/kepa/eu/eu_bdt_models/eu_bdt_lemmatizer.pt',\n",
    "                           'depparse_model_path': '/home/kepa/eu/eu_bdt_models/eu_bdt_parser.pt',\n",
    "                           'depparse_pretrain_path': '/home/kepa/eu/eu_bdt_models/eu_bdt.pretrain.pt'\n",
    "                           }\n",
    "                self.parser = stanfordnlp.Pipeline(**config)\n",
    "                \n",
    "            elif self.lang.lower() == \"english\":\n",
    "                print(\"-------------You are going to use English model-------------\")\n",
    "                config = {'processors': 'tokenize,mwt,pos,lemma,depparse', # Comma-separated list of processors to use\n",
    "                            'lang': 'en', # Language code for the language to build the Pipeline in\n",
    "                            'tokenize_model_path': '/home/kepa/en/en_ewt_models/en_ewt_tokenizer.pt', # Processor-specific arguments are set with keys \"{processor_name}_{argument_name}\"\n",
    "                            #'mwt_model_path': './fr_gsd_models/fr_gsd_mwt_expander.pt',\n",
    "                            'pos_model_path': '/home/kepa/en/en_ewt_models/en_ewt_tagger.pt',\n",
    "                            'pos_pretrain_path': '/home/kepa/en/en_ewt_models/en_ewt.pretrain.pt',\n",
    "                            'lemma_model_path': '/home/kepa/en/en_ewt_models/en_ewt_lemmatizer.pt',\n",
    "                            'depparse_model_path': '/home/kepa/en/en_ewt_models/en_ewt_parser.pt',\n",
    "                            'depparse_pretrain_path': '/home/kepa/en/en_ewt_models/en_ewt.pretrain.pt'\n",
    "                            }\n",
    "                self.parser = stanfordnlp.Pipeline(**config)\n",
    "            elif self.lang.lower() == \"spanish\":\n",
    "                print(\"-------------You are going to use Spanish model-------------\")\n",
    "                config = {'processors': 'tokenize,pos,lemma,depparse', # Comma-separated list of processors to use\n",
    "                            'lang': 'es', # Language code for the language to build the Pipeline in\n",
    "                            'tokenize_model_path': '/home/kepa/es/es_ancora_models/es_ancora_tokenizer.pt', # Processor-specific arguments are set with keys \"{processor_name}_{argument_name}\"\n",
    "                            'pos_model_path': '/home/kepa/es/es_ancora_models/es_ancora_tagger.pt',\n",
    "                            'pos_pretrain_path': '/home/kepa/es/es_ancora_models/es_ancora.pretrain.pt',\n",
    "                            'lemma_model_path': '/home/kepa/es/es_ancora_models/es_ancora_lemmatizer.pt',\n",
    "                            'depparse_model_path': '/home/kepa/es/es_ancora_models/es_ancora_parser.pt',\n",
    "                            'depparse_pretrain_path': '/home/kepa/es/es_ancora_models/es_ancora.pretrain.pt'\n",
    "                            }\n",
    "                self.parser = stanfordnlp.Pipeline(**config)\n",
    "            else:\n",
    "                print(\"........You cannot use this language...........\")\n",
    "        elif self.lib.lower() == \"cube\":\n",
    "            print(\"-----------You are going to use Cube Library-----------\")\n",
    "            if self.lang.lower() == \"basque\":\n",
    "                # initialize it\n",
    "                cube = Cube(verbose=True)\n",
    "                #load(self, language_code, version=\"latest\",local_models_repository=None, \n",
    "                #local_embeddings_file=None, tokenization=True, compound_word_expanding=False, \n",
    "                #tagging=True, lemmatization=True, parsing=True).\n",
    "                #Ejemplo:load(\"es\",tokenization=False, parsing=False)\n",
    "                ## select the desired language (it will auto-download the model on first run)\n",
    "                cube.load(\"eu\",\"latest\")\n",
    "            elif self.lang.lower() == \"english\":\n",
    "                cube = Cube(verbose=True)\n",
    "                cube.load(\"en\",\"latest\")\n",
    "            elif self.lang.lower() == \"spanish\":\n",
    "                cube=Cube(verbose=True)\n",
    "                cube.load(\"es\",\"latest\")\n",
    "            else:\n",
    "                print(\"........You cannot use this language...........\")\n",
    "        else:\n",
    "            print(\"You cannot use this library. Introduce a valid library (Cube or Stanford)\")\n",
    "\n",
    "    def process_text(self, text):\n",
    "        self.text = text.replace('\\n', '@')\n",
    "        self.text = re.sub(r'@+', '@', self.text)\n",
    "        return self.text\n",
    "\n",
    "    '''\n",
    "    Transform data into a unified structure.\n",
    "    '''\n",
    "    \n",
    "    def get_estructure(self, text):\n",
    "        self.text = text\n",
    "        #Loading a text with paragraphs\n",
    "        self.textwithparagraphs = self.process_text(self.text)\n",
    "        #Getting a unified structure [ [sentences], [sentences], ...]\n",
    "        return self.adapt_nlp_model()\n",
    "\n",
    "    def adapt_nlp_model(self):\n",
    "        ma = ModelAdapter(self.parser, self.lib)\n",
    "        return ma.model_analysis(self.textwithparagraphs)\n",
    "\n",
    "\n",
    "\n",
    "\"This is a Singleton class which is going to start necessary classes and methods.\"\n",
    "\n",
    "#from packageDev.Charger import NLPCharger\n",
    "#import re\n",
    "\n",
    "class Main(object):\n",
    "\n",
    "    __instance = None\n",
    "\n",
    "    def __new__(cls):\n",
    "        if Main.__instance is None:\n",
    "            Main.__instance = object.__new__(cls)\n",
    "        return Main.__instance\n",
    "\n",
    "    def start(self):\n",
    "        language=\"basque\"\n",
    "        model=\"cube\"\n",
    "        cargador = NLPCharger(language, model)\n",
    "        cargador.download_model()\n",
    "        cargador.load_model()\n",
    "        if language==\"basque\":\n",
    "            text = \"Kepa hondartzan egon da. Eguraldi oso ona egin zuen.\\nHurrengo astean mendira joango da. \" \\\n",
    "               \"\\n\\nBere lagunak saskibaloi partidu bat antolatu dute 18etan, baina berak ez du jolastuko. \\n \" \\\n",
    "               \"Etor zaitez etxera.\\n Nik egin beharko nuke lan hori. \\n Gizonak liburua galdu du. \\n Irten hortik!\" \\\n",
    "                   \"\\n Emadazu ur botila! \\n Zu beti adarra jotzen.\"\n",
    "        if language==\"english\":\n",
    "            text = \"Kepa is going to the beach. I am Kepa. \\n\" \\\n",
    "                \"Eder is going too. He is Eder.\"\n",
    "        if language==\"spanish\":\n",
    "            text = \"Kepa va ir a la playa. Yo soy Kepa. \\n\" \\\n",
    "                \"Ibon tambien va a ir. El es Ibon.\"\n",
    "        \n",
    "        document = cargador.get_estructure(text)\n",
    "        indicators = document.get_indicators()\n",
    "        printer = Printer(indicators)\n",
    "        printer.print_info()\n",
    "\n",
    "\n",
    "main = Main()\n",
    "main.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
